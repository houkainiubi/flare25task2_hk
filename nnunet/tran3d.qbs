#!/bin/bash
#PBS -N tran3d
#PBS -l nodes=node5:ppn=8
export STUDENT_PLANS_NAME="nnUNetPlans"  # å¿«é€Ÿç‰ˆæœ¬ï¼ˆæ›´å¤§spacingï¼‰

# ğŸ”§ è®­ç»ƒä¼˜åŒ–å‚æ•°
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"  # ä¼˜åŒ–GPUå†…å­˜åˆ†é…
export OMP_NUM_THREADS="4"                               # é™åˆ¶CPUçº¿ç¨‹æ•°

# ğŸ“Š spacingä¼˜åŒ–è¯´æ˜:
# å¿«é€Ÿç‰ˆæœ¬spacingå˜åŒ–:
#   3d_lowres:  [1.97â†’2.5, 1.54â†’2.0, 1.54â†’2.0] mm  (~27%é€Ÿåº¦æå‡)
#   3d_fullres: [1.0â†’1.5, 0.78â†’1.2, 0.78â†’1.2] mm   (~35%é€Ÿåº¦æå‡)
# é¢„è®¡æ€»ä½“æ¨ç†åŠ é€Ÿ: 30-40%ï¼Œç²¾åº¦æŸå¤±: <3%ode5:ppn=8
#PBS -j oe
#PBS -o trani-output.log Â  # åˆå¹¶è¾“å‡ºå’Œé”™è¯¯åˆ°ä¸€ä¸ªæ–‡ä»¶
#PBS -q gentai
cd $PBS_O_WORKDIR
NPROCS=`wc -l < $PBS_NODEFILE`

#dos2unix test.qbså‘½ä»¤è¡Œè¿è¡Œï¼Œæ¶ˆé™¤winæ¢è¡Œç¬¦
# åŠ è½½ Conda ç¯å¢ƒ
source ~/.bashrc
conda activate hk_nnunet_test1
export nnUNet_preprocessed="/home/fanggang_1/hk/nnunet/nnUNet/nnunetv2/DATASET/nnUNet_preprocessed"
export nnUNet_raw="/home/fanggang_1/hk/nnunet/nnUNet/nnunetv2/DATASET/nnUNet_raw"
export nnUNet_results="/home/fanggang_1/hk/nnunet/nnUNet/nnunetv2/DATASET/nnUNet_results"

# =====================================================
# äºŒæ¬¡è½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒç¯å¢ƒå˜é‡è®¾ç½® (5å±‚â†’4å±‚)
# =====================================================

# ğŸ“ğŸ“ äºŒæ¬¡è’¸é¦ï¼šæ•™å¸ˆæ¨¡å‹é…ç½® (ä½¿ç”¨ç¬¬ä¸€é˜¶æ®µçš„5å±‚è½»é‡çº§æ¨¡å‹)
export TEACHER_PLANS_PATH="/home/fanggang_1/hk/nnunet/nnUNet/nnunetv2/DATASET/nnUNet_results/midstudent/nnUNetPlans.json"
export TEACHER_CONFIGURATION="3d_lowres"
export TEACHER_CHECKPOINT="/home/fanggang_1/hk/nnunet/nnUNet/nnunetv2/DATASET/nnUNet_results/Dataset015_flare25/nnUNetTrainerLightweightDistillation__nnUNetPlans__3d_lowres/fold_0/checkpoint_final.pth"
export TEACHER_FOLD="0"

# ğŸ¯ äºŒæ¬¡è’¸é¦å‚æ•° (é’ˆå¯¹4å±‚è¶…è½»é‡çº§æ¨¡å‹ä¼˜åŒ–)
export DISTILL_LOSS_TYPE="progressive_kl"  # æ¸è¿›å¼è’¸é¦æŸå¤±
export DISTILL_TEMPERATURE_2ND="2.0"       # äºŒæ¬¡è’¸é¦æ¸©åº¦ (é™ä½ä»¥ä¿æŒæ€§èƒ½)
export DISTILL_ALPHA_2ND="0.8"             # äºŒæ¬¡è’¸é¦æƒé‡ (æé«˜)
export DISTILL_BETA_2ND="0.2"              # ä»»åŠ¡æŸå¤±æƒé‡ (é™ä½)

# âš¡ è½»é‡çº§ç½‘ç»œæ ‡è¯†
export NNUNET_USE_LIGHTWEIGHT="true"       # å¯ç”¨è½»é‡çº§æ¶æ„

# ğŸ—ï¸  4å±‚è¶…è½»é‡çº§å­¦ç”Ÿæ¨¡å‹Plansæ–‡ä»¶é€‰é¡¹
export STUDENT_PLANS_NAME="nnUNetPlans"  # å¿«é€Ÿç‰ˆæœ¬ï¼ˆæ›´å¤§spacingï¼‰
# export STUDENT_PLANS_NAME="ï¼ˆstudent-4stageï¼‰nnUNetPlans"     # æ ‡å‡†ç‰ˆæœ¬ï¼ˆåŸspacingï¼‰
# export STUDENT_PLANS_NAME="nnUNetPlans"                      # ä½¿ç”¨æ ‡å‡†Plansï¼ˆå·²æ˜¯4å±‚ï¼‰

# ğŸ”§ è®­ç»ƒä¼˜åŒ–å‚æ•°
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"  # ä¼˜åŒ–GPUå†…å­˜åˆ†é…
export OMP_NUM_THREADS="4"                               # é™åˆ¶CPUçº¿ç¨‹æ•°

# å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ç‰¹æ®Šé‡å®šå‘æ–¹å¼å®æ—¶è¾“å‡ºæ—¥å¿—
{
    # æ˜¾ç¤ºå¼€å§‹æ—¶é—´
    echo "==================== JOB STARTED ===================="
    echo "Start time: $(date)"
    echo "Working directory: $(pwd)"
    echo "Job ID: $PBS_JOBID"
    
    # ğŸ“Š ç³»ç»Ÿä¿¡æ¯
    nvidia-smi
    echo "Python version: $(python --version)"
    echo "PyTorch version: $(python -c 'import torch; print(torch.__version__)' 2>/dev/null || echo 'Not available')"
    
    # ğŸ” éªŒè¯äºŒæ¬¡è’¸é¦ç¯å¢ƒ
    echo "==================== äºŒæ¬¡è’¸é¦ç¯å¢ƒéªŒè¯ ===================="
    echo "æ•™å¸ˆæ¨¡å‹è·¯å¾„: $TEACHER_CHECKPOINT"
    echo "4å±‚å­¦ç”ŸPlans: $STUDENT_PLANS_NAME"
    echo "è’¸é¦æ¸©åº¦: $DISTILL_TEMPERATURE_2ND"
    echo "è’¸é¦æƒé‡ Î±: $DISTILL_ALPHA_2ND, Î²: $DISTILL_BETA_2ND"
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    if [ -f "$TEACHER_CHECKPOINT" ]; then
        echo "âœ… æ•™å¸ˆæ¨¡å‹æ–‡ä»¶å­˜åœ¨"
    else
        echo "âŒ æ•™å¸ˆæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: $TEACHER_CHECKPOINT"
    fi
    
    STUDENT_PLANS_PATH="$nnUNet_preprocessed/Dataset015_flare25/$STUDENT_PLANS_NAME.json"
    if [ -f "$STUDENT_PLANS_PATH" ]; then
        echo "âœ… 4å±‚å­¦ç”ŸPlansæ–‡ä»¶å­˜åœ¨: $STUDENT_PLANS_NAME"
        echo "   è·¯å¾„: $STUDENT_PLANS_PATH"
    else
        echo "âŒ 4å±‚å­¦ç”ŸPlansæ–‡ä»¶ä¸å­˜åœ¨: $STUDENT_PLANS_PATH"
        echo "   è¯·ç¡®ä¿å·²åˆ›å»ºå¯¹åº”çš„Plansé…ç½®æ–‡ä»¶"
    fi
    echo "==========================================================="
    
    # æ·»åŠ æ ‡å‡†é”™è¯¯è¾“å‡ºåˆ°æ—¥å¿—
    echo "Running nnUNet äºŒæ¬¡è’¸é¦è®­ç»ƒ with real-time logging..."
    
    # ä½¿ç”¨ exec é‡å®šå‘æ‰€æœ‰åç»­å‘½ä»¤çš„è¾“å‡º
    exec > >(tee -a trani-output.log) 2>&1
    
    # è¾“å…¥éœ€è¦æ‰§è¡Œçš„å‘½ä»¤
    #nnUNetv2_plan_and_preprocess -d 15 --verify_dataset_integrity
    export CUDA_VISIBLE_DEVICES=0,1
    
    # ===============================
    # è®­ç»ƒé€‰é¡¹ (è¯·æ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Š)
    # ===============================
    
    # ğŸ“ğŸ“ é€‰é¡¹1: äºŒæ¬¡è½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒ (5å±‚â†’4å±‚è¶…è½»é‡çº§) - å¿«é€Ÿç‰ˆæœ¬
    # ä½¿ç”¨ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå¥½çš„5å±‚è½»é‡çº§æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œè®­ç»ƒ4å±‚è¶…è½»é‡çº§å­¦ç”Ÿæ¨¡å‹
    # å¿«é€Ÿç‰ˆæœ¬ï¼šæ›´å¤§spacingï¼Œé¢„è®¡åŠ é€Ÿ30-40%ï¼Œbatch_sizeæå‡è‡³8
    nnUNetv2_train 15 3d_lowres 0 -tr nnUNetTrainerLightweightDistillation2nd -p "$STUDENT_PLANS_NAME" 
    
    # ğŸ“ é€‰é¡¹2: ç¬¬ä¸€é˜¶æ®µè½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒ (6å±‚â†’5å±‚) [å·²å®Œæˆ]
    #nnUNetv2_train 15 3d_lowres 0 -tr nnUNetTrainerLightweightDistillation --c
    
    # âš¡ é€‰é¡¹3: çº¯è½»é‡çº§è®­ç»ƒ (æ— è’¸é¦)
    #nnUNetv2_train 15 3d_lowres 0 -tr nnUNetTrainerLightweight --c
    
    # ğŸ”„ é€‰é¡¹4: åŸå§‹ä½™å¼¦é€€ç«è®­ç»ƒ
    #nnUNetv2_train 15 3d_lowres 0 -num_gpus 2 -tr nnUNetTrainerCosAnneal --c

    
    # æ˜¾ç¤ºç»“æŸæ—¶é—´å’Œé€€å‡ºçŠ¶æ€
    echo "Exit status: $?"
    echo "End time: $(date)"
    echo "==================== JOB FINISHED ===================="
} | tee -a trani-output.log
