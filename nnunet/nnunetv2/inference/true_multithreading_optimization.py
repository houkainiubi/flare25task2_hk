"""
çœŸæ­£çš„å¤šçº¿ç¨‹åå¤„ç†ä¼˜åŒ–
åœ¨åº”ç”¨å±‚é¢å®ç°å¹¶è¡Œé‡é‡‡æ ·å’Œæ–‡ä»¶ä¿å­˜
"""
import os
import time
import torch
import numpy as np
import psutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Union, List, Tuple
from functools import partial
import threading
from queue import Queue
from batchgenerators.utilities.file_and_folder_operations import save_pickle
from scipy.ndimage import zoom
from skimage.transform import resize


def parallel_resample_prediction(prediction_logits: np.ndarray, 
                                original_shape: Tuple[int, ...], 
                                properties_dict: dict,
                                num_workers: int = None) -> np.ndarray:
    """
    å¹¶è¡ŒåŒ–é‡é‡‡æ ·ï¼šå°†é¢„æµ‹ç»“æœä»ä½åˆ†è¾¨ç‡é‡é‡‡æ ·å›åŸå§‹åˆ†è¾¨ç‡
    è¿™æ˜¯Step 3ä¸­æœ€è€—æ—¶çš„æ“ä½œ
    
    Args:
        prediction_logits: é¢„æµ‹çš„logitsæ•°ç»„ [classes, z, y, x]
        original_shape: åŸå§‹å›¾åƒå½¢çŠ¶
        properties_dict: åŒ…å«spacingç­‰ä¿¡æ¯çš„å±æ€§å­—å…¸
        num_workers: å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        
    Returns:
        é‡é‡‡æ ·åçš„é¢„æµ‹ç»“æœ
    """
    if num_workers is None:
        num_workers = min(psutil.cpu_count(logical=False), 8)
    
    print(f"ğŸ”„ ä½¿ç”¨ {num_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œé‡é‡‡æ ·...")
    
    # è·å–é‡é‡‡æ ·å‚æ•°
    current_shape = prediction_logits.shape[1:]  # å»æ‰classesç»´åº¦
    target_shape = original_shape
    
    # è®¡ç®—ç¼©æ”¾å› å­
    zoom_factors = [t/c for t, c in zip(target_shape, current_shape)]
    
    num_classes = prediction_logits.shape[0]
    
    # ä¸ºæ¯ä¸ªç±»åˆ«å¹¶è¡Œæ‰§è¡Œé‡é‡‡æ ·
    def resample_single_class(class_idx):
        """é‡é‡‡æ ·å•ä¸ªç±»åˆ«çš„é¢„æµ‹"""
        class_prediction = prediction_logits[class_idx]
        
        # ä½¿ç”¨scipyçš„zoomè¿›è¡Œé‡é‡‡æ ·ï¼ˆæ¯”torchæ›´å¿«ï¼‰
        resampled = zoom(class_prediction, zoom_factors, order=1, prefilter=False)
        
        return class_idx, resampled
    
    # å¹¶è¡Œæ‰§è¡Œé‡é‡‡æ ·
    resampled_predictions = np.zeros((num_classes, *target_shape), dtype=prediction_logits.dtype)
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        # æäº¤æ‰€æœ‰é‡é‡‡æ ·ä»»åŠ¡
        future_to_class = {
            executor.submit(resample_single_class, i): i 
            for i in range(num_classes)
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_class):
            class_idx, resampled_data = future.result()
            resampled_predictions[class_idx] = resampled_data
    
    print("âœ… å¹¶è¡Œé‡é‡‡æ ·å®Œæˆ")
    return resampled_predictions


def parallel_segmentation_postprocess(resampled_logits: np.ndarray, 
                                    properties_dict: dict,
                                    label_manager,
                                    num_workers: int = None) -> np.ndarray:
    """
    å¹¶è¡ŒåŒ–åˆ†å‰²åå¤„ç†ï¼šå°†logitsè½¬æ¢ä¸ºæœ€ç»ˆåˆ†å‰²ç»“æœ
    åŒ…æ‹¬softmaxã€argmaxç­‰æ“ä½œ
    """
    if num_workers is None:
        num_workers = min(psutil.cpu_count(logical=False), 4)
    
    print(f"ğŸ”„ ä½¿ç”¨ {num_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œåå¤„ç†...")
    
    # åˆ†å—å¤„ç†å¤§ä½“ç§¯æ•°æ®
    def process_chunk(chunk_data, start_slice, end_slice):
        """å¤„ç†æ•°æ®å—"""
        # åº”ç”¨softmax
        chunk_softmax = torch.softmax(torch.from_numpy(chunk_data), dim=0).numpy()
        
        # è·å–æœ€ç»ˆåˆ†å‰²ï¼ˆargmaxï¼‰
        chunk_segmentation = np.argmax(chunk_softmax, axis=0)
        
        # åº”ç”¨æ ‡ç­¾æ˜ å°„
        if hasattr(label_manager, 'all_labels'):
            for i, label in enumerate(label_manager.all_labels):
                if label != i:  # éœ€è¦é‡æ–°æ˜ å°„
                    chunk_segmentation[chunk_segmentation == i] = label
        
        return start_slice, end_slice, chunk_segmentation
    
    # æŒ‰Zè½´åˆ†å—å¹¶è¡Œå¤„ç†
    z_dim = resampled_logits.shape[1]  # å‡è®¾å½¢çŠ¶æ˜¯ [classes, z, y, x]
    chunk_size = max(1, z_dim // num_workers)
    
    final_segmentation = np.zeros(resampled_logits.shape[1:], dtype=np.uint8)
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = []
        
        for start_z in range(0, z_dim, chunk_size):
            end_z = min(start_z + chunk_size, z_dim)
            chunk = resampled_logits[:, start_z:end_z, :, :]
            
            future = executor.submit(process_chunk, chunk, start_z, end_z)
            futures.append(future)
        
        # ç»„è£…ç»“æœ
        for future in as_completed(futures):
            start_slice, end_slice, chunk_result = future.result()
            final_segmentation[start_slice:end_slice] = chunk_result
    
    print("âœ… å¹¶è¡Œåå¤„ç†å®Œæˆ")
    return final_segmentation


def parallel_file_operations(segmentation: np.ndarray,
                           probabilities: np.ndarray,
                           output_file_truncated: str,
                           dataset_json_dict: dict,
                           properties_dict: dict,
                           plans_manager,
                           save_probabilities: bool = False) -> None:
    """
    å¹¶è¡ŒåŒ–æ–‡ä»¶ä¿å­˜æ“ä½œ
    """
    print("ğŸ”„ å¹¶è¡Œä¿å­˜æ–‡ä»¶...")
    
    def save_segmentation():
        """ä¿å­˜åˆ†å‰²ç»“æœ"""
        try:
            rw = plans_manager.image_reader_writer_class()
            output_fname = output_file_truncated + dataset_json_dict['file_ending']
            rw.write_seg(segmentation, output_fname, properties_dict)
            print("âœ… åˆ†å‰²æ–‡ä»¶ä¿å­˜å®Œæˆ")
        except Exception as e:
            print(f"âŒ åˆ†å‰²æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
    
    def save_probabilities_data():
        """ä¿å­˜æ¦‚ç‡æ•°æ®"""
        try:
            np.savez_compressed(output_file_truncated + '.npz', probabilities=probabilities)
            save_pickle(properties_dict, output_file_truncated + '.pkl')
            print("âœ… æ¦‚ç‡æ–‡ä»¶ä¿å­˜å®Œæˆ")
        except Exception as e:
            print(f"âŒ æ¦‚ç‡æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
    
    # å¹¶è¡Œæ‰§è¡Œæ–‡ä»¶ä¿å­˜
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = [executor.submit(save_segmentation)]
        
        if save_probabilities and probabilities is not None:
            futures.append(executor.submit(save_probabilities_data))
        
        # ç­‰å¾…æ‰€æœ‰ä¿å­˜æ“ä½œå®Œæˆ
        for future in as_completed(futures):
            future.result()  # è¿™ä¼šæŠ›å‡ºä»»ä½•å¼‚å¸¸


def optimized_export_prediction_from_logits(predicted_array_or_file: Union[np.ndarray, torch.Tensor], 
                                           properties_dict: dict,
                                           configuration_manager, 
                                           plans_manager, 
                                           dataset_json_dict_or_file: Union[dict, str], 
                                           output_file_truncated: str,
                                           save_probabilities: bool = False,
                                           enable_parallel_resample: bool = True,
                                           resample_workers: int = None) -> None:
    """
    å®Œå…¨ä¼˜åŒ–çš„é¢„æµ‹å¯¼å‡ºå‡½æ•°
    å®ç°çœŸæ­£çš„å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
    """
    print("ğŸš€ å¯åŠ¨æ·±åº¦å¤šçº¿ç¨‹åå¤„ç†ä¼˜åŒ–...")
    
    if isinstance(dataset_json_dict_or_file, str):
        from batchgenerators.utilities.file_and_folder_operations import load_json
        dataset_json_dict_or_file = load_json(dataset_json_dict_or_file)

    label_manager = plans_manager.get_label_manager(dataset_json_dict_or_file)
    
    # è½¬æ¢è¾“å…¥ä¸ºnumpyæ•°ç»„
    if isinstance(predicted_array_or_file, torch.Tensor):
        prediction_logits = predicted_array_or_file.cpu().detach().numpy()
    else:
        prediction_logits = predicted_array_or_file
    
    # è·å–åŸå§‹å½¢çŠ¶
    original_shape = properties_dict['shape_after_cropping']
    
    start_time = time.time()
    
    # æ­¥éª¤1: å¹¶è¡Œé‡é‡‡æ ·ï¼ˆæœ€è€—æ—¶çš„æ“ä½œï¼‰
    if enable_parallel_resample:
        resampled_logits = parallel_resample_prediction(
            prediction_logits, original_shape, properties_dict, resample_workers
        )
    else:
        # å›é€€åˆ°åŸå§‹æ–¹æ³•
        from nnunetv2.inference.export_prediction import convert_predicted_logits_to_segmentation_with_correct_shape
        result = convert_predicted_logits_to_segmentation_with_correct_shape(
            prediction_logits, plans_manager, configuration_manager, label_manager, 
            properties_dict, return_probabilities=save_probabilities
        )
        if save_probabilities:
            segmentation, probabilities = result
        else:
            segmentation = result
            probabilities = None
        
        # å¹¶è¡Œä¿å­˜æ–‡ä»¶
        parallel_file_operations(
            segmentation, probabilities, output_file_truncated, 
            dataset_json_dict_or_file, properties_dict, plans_manager, save_probabilities
        )
        return
    
    resample_time = time.time() - start_time
    print(f"é‡é‡‡æ ·è€—æ—¶: {resample_time:.2f}s")
    
    # æ­¥éª¤2: å¹¶è¡Œåå¤„ç†
    start_time = time.time()
    segmentation = parallel_segmentation_postprocess(
        resampled_logits, properties_dict, label_manager
    )
    postprocess_time = time.time() - start_time
    print(f"åå¤„ç†è€—æ—¶: {postprocess_time:.2f}s")
    
    # æ­¥éª¤3: å‡†å¤‡æ¦‚ç‡æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    probabilities = None
    if save_probabilities:
        start_time = time.time()
        # å¹¶è¡Œè®¡ç®—softmaxæ¦‚ç‡
        probabilities = torch.softmax(torch.from_numpy(resampled_logits), dim=0).numpy()
        prob_time = time.time() - start_time
        print(f"æ¦‚ç‡è®¡ç®—è€—æ—¶: {prob_time:.2f}s")
    
    # æ­¥éª¤4: å¹¶è¡Œæ–‡ä»¶ä¿å­˜
    start_time = time.time()
    parallel_file_operations(
        segmentation, probabilities, output_file_truncated, 
        dataset_json_dict_or_file, properties_dict, plans_manager, save_probabilities
    )
    save_time = time.time() - start_time
    print(f"æ–‡ä»¶ä¿å­˜è€—æ—¶: {save_time:.2f}s")
    
    print("âœ… æ·±åº¦å¤šçº¿ç¨‹åå¤„ç†ä¼˜åŒ–å®Œæˆ")


def print_advanced_optimization_info():
    """
    æ‰“å°é«˜çº§ä¼˜åŒ–ä¿¡æ¯
    """
    physical_cores = psutil.cpu_count(logical=False)
    logical_cores = psutil.cpu_count(logical=True)
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    print("\n" + "="*70)
    print("ğŸš€ é«˜çº§å¤šçº¿ç¨‹åå¤„ç†ä¼˜åŒ–")
    print("="*70)
    print(f"ç‰©ç†CPUæ ¸å¿ƒæ•°: {physical_cores}")
    print(f"é€»è¾‘CPUæ ¸å¿ƒæ•°: {logical_cores}")
    print(f"ç³»ç»Ÿå†…å­˜: {memory_gb:.1f} GB")
    print(f"é‡é‡‡æ ·çº¿ç¨‹æ•°: {min(physical_cores, 8)}")
    print(f"åå¤„ç†çº¿ç¨‹æ•°: {min(physical_cores, 4)}")
    print(f"æ–‡ä»¶I/Oçº¿ç¨‹æ•°: 2")
    print("="*70)
    print("âœ¨ ä¼˜åŒ–ç­–ç•¥:")
    print("  â€¢ å¹¶è¡Œé‡é‡‡æ · - æŒ‰ç±»åˆ«åˆ†å‰²å¤„ç†")
    print("  â€¢ å¹¶è¡Œåå¤„ç† - æŒ‰ç©ºé—´åˆ†å—å¤„ç†") 
    print("  â€¢ å¹¶è¡Œæ–‡ä»¶ä¿å­˜ - åˆ†å‰²å’Œæ¦‚ç‡åŒæ—¶ä¿å­˜")
    print("  â€¢ å†…å­˜ä¼˜åŒ– - åŠæ—¶é‡Šæ”¾ä¸­é—´ç»“æœ")
    print("="*70 + "\n")
