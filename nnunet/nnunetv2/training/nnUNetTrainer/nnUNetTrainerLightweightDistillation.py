import torch
import torch.nn as nn
import torch.nn.functional as F
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from nnunetv2.training.nnUNetTrainer.nnUNetTrainerLightweight import nnUNetTrainerLightweight
from nnunetv2.utilities.plans_handling.plans_handler import PlansManager
from nnunetv2.utilities.label_handling.label_handling import LabelManager
from nnunetv2.paths import nnUNet_preprocessed, nnUNet_results
from batchgenerators.utilities.file_and_folder_operations import join, load_json, isfile
import numpy as np
import os
import logging
from torch.cuda.amp import autocast
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
from nnunetv2.training.loss.compound_losses import DC_and_CE_loss, DC_and_BCE_loss
from nnunetv2.training.loss.deep_supervision import DeepSupervisionWrapper
from nnunetv2.training.loss.dice import MemoryEfficientSoftDiceLoss
from nnunetv2.training.lr_scheduler.polylr import PolyLRScheduler
from nnunetv2.utilities.get_network_from_plans import get_network_from_plans
from nnunetv2.utilities.collate_outputs import collate_outputs
from nnunetv2.utilities.helpers import empty_cache
from torch.optim import SGD
from torch.cuda.amp import GradScaler
from contextlib import contextmanager

# è®¾ç½®è½»é‡çº§è’¸é¦è®­ç»ƒå™¨æ—¥å¿—
distill_logger = logging.getLogger('nnunet.lightweight_distillation')
if not distill_logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('[%(name)s] %(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    distill_logger.addHandler(handler)
    distill_logger.setLevel(logging.INFO)


class nnUNetTrainerLightweightDistillation(nnUNetTrainerLightweight):
    """
    è½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
    
    ç‰¹ç‚¹ï¼š
    1. ç»§æ‰¿è½»é‡çº§è®­ç»ƒå™¨çš„æ‰€æœ‰ä¼˜åŒ–
    2. æ”¯æŒä»æ ‡å‡†/è½»é‡çº§æ•™å¸ˆæ¨¡å‹å­¦ä¹ 
    3. ä½¿ç”¨ç“¶é¢ˆæ®‹å·®å— + é‡‘å­—å¡”æ± åŒ–çš„å­¦ç”Ÿç½‘ç»œ
    4. é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„è’¸é¦æŸå¤±ä¼˜åŒ–
    """
    
    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict,
                 device: torch.device = torch.device('cuda')):
        """
        è½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨ - é€šè¿‡ç¯å¢ƒå˜é‡è·å–é…ç½®
        """
        distill_logger.info("ğŸ“ åˆå§‹åŒ–è½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨")
        
        # é¦–å…ˆåˆå§‹åŒ–è½»é‡çº§è®­ç»ƒå™¨ï¼ˆç¡®ä¿è½»é‡çº§ç½‘ç»œæ„å»ºï¼‰
        super().__init__(plans, configuration, fold, dataset_json, device)
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–æ•™å¸ˆæ¨¡å‹é…ç½®
        self.teacher_plans_path = os.environ.get('TEACHER_PLANS_PATH', None)
        self.teacher_configuration = os.environ.get('TEACHER_CONFIGURATION', '3d_fullres')
        self.teacher_checkpoint = os.environ.get('TEACHER_CHECKPOINT', None)
        self.teacher_fold = os.environ.get('TEACHER_FOLD', 'all')
        
        # è’¸é¦å‚æ•°
        self.distill_loss_type = os.environ.get('DISTILL_LOSS_TYPE', 'kl')
        self.distill_temperature = float(os.environ.get('DISTILL_TEMPERATURE', '3.0'))
        self.distill_alpha = float(os.environ.get('DISTILL_ALPHA', '0.7'))
        self.distill_beta = float(os.environ.get('DISTILL_BETA', '0.3'))
        
        # æ£€æŸ¥å¿…è¦å‚æ•°æ˜¯å¦è®¾ç½®
        if not self.teacher_plans_path or not self.teacher_checkpoint:
            distill_logger.error("âŒ æ•™å¸ˆæ¨¡å‹è·¯å¾„å’Œæ£€æŸ¥ç‚¹å¿…é¡»é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®")
            raise ValueError("æ•™å¸ˆæ¨¡å‹è·¯å¾„å’Œæ£€æŸ¥ç‚¹å¿…é¡»é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®")
        
        distill_logger.info(f"ğŸ“š æ•™å¸ˆæ¨¡å‹é…ç½®:")
        distill_logger.info(f"   Plans: {self.teacher_plans_path}")
        distill_logger.info(f"   Checkpoint: {self.teacher_checkpoint}")
        distill_logger.info(f"   Configuration: {self.teacher_configuration}")
        distill_logger.info(f"   Fold: {self.teacher_fold}")
        
        distill_logger.info(f"ğŸ¯ è’¸é¦å‚æ•°:")
        distill_logger.info(f"   æŸå¤±ç±»å‹: {self.distill_loss_type}")
        distill_logger.info(f"   æ¸©åº¦: {self.distill_temperature}")
        distill_logger.info(f"   Alpha: {self.distill_alpha}")
        distill_logger.info(f"   Beta: {self.distill_beta}")
        
        # æ•™å¸ˆæ¨¡å‹å°†åœ¨initializeæ–¹æ³•ä¸­åŠ è½½
        self.teacher_model = None
        self.teacher_plans_manager = None
        self.teacher_configuration_manager = None

    def initialize(self):
        """åˆå§‹åŒ–å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹"""
        distill_logger.info("ğŸ”§ åˆå§‹åŒ–è½»é‡çº§è’¸é¦è®­ç»ƒå™¨...")
        
        # é¦–å…ˆåˆå§‹åŒ–è½»é‡çº§å­¦ç”Ÿæ¨¡å‹
        super().initialize()
        
        # è®°å½•å­¦ç”Ÿç½‘ç»œä¿¡æ¯
        self._log_student_network_info()
        
        # ç„¶ååŠ è½½æ•™å¸ˆæ¨¡å‹
        self._load_teacher_model()
        
        # åˆå§‹åŒ–è’¸é¦æŸå¤±å‡½æ•°
        self.distill_loss = LightweightKnowledgeDistillationLoss(
            loss_type=self.distill_loss_type,
            temperature=self.distill_temperature,
            alpha=self.distill_alpha,
            beta=self.distill_beta
        )
        
        distill_logger.info("âœ… è½»é‡çº§è’¸é¦è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    def _log_student_network_info(self):
        """è®°å½•å­¦ç”Ÿç½‘ç»œä¿¡æ¯"""
        if hasattr(self.network, '__class__'):
            network_class = self.network.__class__.__name__
            distill_logger.info(f"ğŸ“ å­¦ç”Ÿç½‘ç»œ: {network_class}")
            
            # ç»Ÿè®¡å‚æ•°
            total_params = sum(p.numel() for p in self.network.parameters())
            trainable_params = sum(p.numel() for p in self.network.parameters() if p.requires_grad)
            
            distill_logger.info(f"ğŸ“Š å­¦ç”Ÿç½‘ç»œå‚æ•°ç»Ÿè®¡:")
            distill_logger.info(f"   æ€»å‚æ•°: {total_params:,}")
            distill_logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            distill_logger.info(f"   å‚æ•°é‡: {total_params/1e6:.2f}M")

    def _load_teacher_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹ - æ”¯æŒæ ‡å‡†å’Œè½»é‡çº§æ•™å¸ˆ"""
        distill_logger.info("ğŸ“š åŠ è½½æ•™å¸ˆæ¨¡å‹...")
        
        try:
            # åŠ è½½æ•™å¸ˆæ¨¡å‹çš„è®¡åˆ’æ–‡ä»¶
            teacher_plans = load_json(self.teacher_plans_path)
            self.teacher_plans_manager = PlansManager(teacher_plans)
            self.teacher_configuration_manager = self.teacher_plans_manager.get_configuration(
                self.teacher_configuration)
            
            # è·å–æ•™å¸ˆæ¨¡å‹çš„ç½‘ç»œæ¶æ„å‚æ•°
            teacher_arch_class_name = self.teacher_configuration_manager.network_arch_class_name
            teacher_arch_kwargs = self.teacher_configuration_manager.network_arch_init_kwargs
            teacher_arch_kwargs_req_import = self.teacher_configuration_manager.network_arch_init_kwargs_req_import
            
            # æ ¹æ®æ•°æ®é›†è°ƒæ•´è¾“å…¥è¾“å‡ºé€šé“
            num_input_channels = determine_num_input_channels(
                self.teacher_plans_manager, self.teacher_configuration_manager, self.dataset_json)
            label_manager = self.teacher_plans_manager.get_label_manager(self.dataset_json)
            num_output_channels = label_manager.num_segmentation_heads
            
            distill_logger.info(f"ğŸ—ï¸  æ•™å¸ˆç½‘ç»œæ¶æ„: {teacher_arch_class_name}")
            distill_logger.info(f"ğŸ“ è¾“å…¥é€šé“: {num_input_channels}, è¾“å‡ºé€šé“: {num_output_channels}")
            
            # æ£€æµ‹æ•™å¸ˆæ¨¡å‹æ˜¯å¦åº”è¯¥æ˜¯è½»é‡çº§çš„
            self._detect_and_setup_teacher_model_type()
            
            # åˆ›å»ºæ•™å¸ˆæ¨¡å‹
            self.teacher_model = get_network_from_plans(
                teacher_arch_class_name,
                teacher_arch_kwargs,
                teacher_arch_kwargs_req_import,
                num_input_channels,
                num_output_channels,
                deep_supervision=self.enable_deep_supervision
            ).to(self.device)
            
            # åŠ è½½æƒé‡
            checkpoint = torch.load(self.teacher_checkpoint, map_location='cpu', weights_only=False)
            
            if 'network_weights' in checkpoint:
                teacher_weights = checkpoint['network_weights']
                teacher_trainer_name = checkpoint.get('trainer_name', 'Unknown')
                distill_logger.info(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆè®­ç»ƒå™¨: {teacher_trainer_name}")
            else:
                teacher_weights = checkpoint
                teacher_trainer_name = 'Unknown'
            
            # å¤„ç†æ¨¡å‹æƒé‡é”®åï¼Œç§»é™¤å¯èƒ½çš„ _orig_mod. å‰ç¼€
            cleaned_weights = {}
            for key, value in teacher_weights.items():
                # ç§»é™¤ _orig_mod. å‰ç¼€ï¼ˆtorch.compile æˆ– DDP äº§ç”Ÿçš„ï¼‰
                clean_key = key.replace('_orig_mod.', '') if key.startswith('_orig_mod.') else key
                cleaned_weights[clean_key] = value
            
            distill_logger.info(f"ğŸ”§ æ¸…ç†æƒé‡é”®å: {len(teacher_weights)} -> {len(cleaned_weights)} é”®")
            
            self.teacher_model.load_state_dict(cleaned_weights)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®è¯„ä¼°æ¨¡å¼
            self.teacher_model.to(self.device)
            self.teacher_model.eval()
            
            # ç»Ÿè®¡æ•™å¸ˆæ¨¡å‹å‚æ•°
            teacher_total_params = sum(p.numel() for p in self.teacher_model.parameters())
            distill_logger.info(f"ğŸ“Š æ•™å¸ˆç½‘ç»œå‚æ•°: {teacher_total_params:,} ({teacher_total_params/1e6:.2f}M)")
            
            # å¦‚æœæ˜¯DDPç¯å¢ƒï¼ŒåŒ…è£…æ•™å¸ˆæ¨¡å‹
            if self.is_ddp:
                self.teacher_model = DDP(self.teacher_model, device_ids=[self.local_rank])
            
            distill_logger.info(f"âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½å®Œæˆ: {self.teacher_configuration}")
            
        except Exception as e:
            distill_logger.error(f"âŒ æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def _detect_and_setup_teacher_model_type(self):
        """æ£€æµ‹å¹¶è®¾ç½®æ•™å¸ˆæ¨¡å‹ç±»å‹"""
        # æ£€æŸ¥checkpointæ˜¯å¦åŒ…å«è½»é‡çº§ç‰¹å¾
        try:
            checkpoint = torch.load(self.teacher_checkpoint, map_location='cpu', weights_only=False)
            
            if 'network_weights' in checkpoint:
                params = checkpoint['network_weights']
                trainer_name = checkpoint.get('trainer_name', '')
            else:
                params = checkpoint
                trainer_name = ''
            
            # æ£€æµ‹è½»é‡çº§ç‰¹å¾
            lightweight_signatures = [
                'bottleneck_compress', 'bottleneck_expand', 'bn_compress',
                'pyramid_pool', 'stages.0.1.weight', 'final_conv.weight'
            ]
            
            depthwise_features = sum(1 for key in params.keys() 
                                   if 'depthwise.weight' in key or 'pointwise.weight' in key)
            lightweight_features = sum(1 for key in params.keys() 
                                     for sig in lightweight_signatures if sig in key)
            
            is_lightweight_teacher = (lightweight_features > 0 or 
                                    depthwise_features > 10 or 
                                    'Lightweight' in trainer_name)
            
            if is_lightweight_teacher:
                distill_logger.info("ğŸ¯ æ£€æµ‹åˆ°è½»é‡çº§æ•™å¸ˆæ¨¡å‹ï¼Œå¯ç”¨è½»é‡çº§æ¨¡å¼æ„å»ºæ•™å¸ˆç½‘ç»œ")
                os.environ['NNUNET_USE_LIGHTWEIGHT'] = 'true'
            else:
                distill_logger.info("ğŸ›ï¸  æ£€æµ‹åˆ°æ ‡å‡†æ•™å¸ˆæ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼æ„å»ºæ•™å¸ˆç½‘ç»œ")
                # ä¸´æ—¶ç¦ç”¨è½»é‡çº§æ¨¡å¼æ„å»ºæ•™å¸ˆç½‘ç»œ
                old_flag = os.environ.get('NNUNET_USE_LIGHTWEIGHT', '')
                os.environ.pop('NNUNET_USE_LIGHTWEIGHT', None)
                # ç¨åä¼šæ¢å¤
                self._restore_lightweight_flag = old_flag
                
        except Exception as e:
            distill_logger.warning(f"âš ï¸  æ— æ³•æ£€æµ‹æ•™å¸ˆæ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®: {e}")

    def train_step(self, batch: dict) -> dict:
        """è®­ç»ƒæ­¥éª¤ - è½»é‡çº§çŸ¥è¯†è’¸é¦"""
        data = batch['data']
        target = batch['target']

        data = data.to(self.device, non_blocking=True)
        if isinstance(target, list):
            target = [i.to(self.device, non_blocking=True) for i in target]
        else:
            target = target.to(self.device, non_blocking=True)

        self.optimizer.zero_grad(set_to_none=True)
        
        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆè½»é‡çº§ç½‘ç»œï¼‰
        with autocast(enabled=(self.device.type == 'cuda')) if self.device.type == 'cuda' else dummy_context():
            student_output = self.network(data)
            
            # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ (æ— æ¢¯åº¦)
            with torch.no_grad():
                teacher_output = self.teacher_model(data)
            
            # è®¡ç®—è’¸é¦æŸå¤±
            l = self.distill_loss(student_output, teacher_output, target)

        if self.grad_scaler is not None:
            self.grad_scaler.scale(l).backward()
            self.grad_scaler.unscale_(self.optimizer)
            # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢NaN
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)  # ä»12é™åˆ°1.0
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰NaN/Inf
            for param in self.network.parameters():
                if param.grad is not None:
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        param.grad.zero_()  # æ¸…é›¶æœ‰é—®é¢˜çš„æ¢¯åº¦
            
            self.grad_scaler.step(self.optimizer)
            self.grad_scaler.update()
        else:
            l.backward()
            # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)
            
            # æ£€æŸ¥æ¢¯åº¦
            for param in self.network.parameters():
                if param.grad is not None:
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        param.grad.zero_()
            
            self.optimizer.step()
            
        return {'loss': l.detach().cpu().numpy()}

    def configure_optimizers(self):
        """ä¸ºè½»é‡çº§å­¦ç”Ÿæ¨¡å‹é…ç½®ä¼˜åŒ–å™¨"""
        distill_logger.info("âš™ï¸  é…ç½®è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ä¼˜åŒ–å™¨")
        optimizer = SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay,
                        momentum=0.99, nesterov=True)
        lr_scheduler = PolyLRScheduler(optimizer, self.initial_lr, self.num_epochs)
        return optimizer, lr_scheduler

    def on_train_end(self):
        """è®­ç»ƒç»“æŸæ—¶ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        distill_logger.info("ğŸ“ è½»é‡çº§è’¸é¦è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹")
        super().on_train_end()
        self.save_checkpoint(join(self.output_folder, "lightweight_distilled_model_final.pth"))

    def save_checkpoint(self, filename: str) -> None:
        """ä¿å­˜è½»é‡çº§è’¸é¦æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if self.local_rank == 0:
            if not self.disable_checkpointing:
                if self.is_ddp:
                    mod = self.network.module
                else:
                    mod = self.network
                
                if isinstance(mod, OptimizedModule):
                    mod = mod._orig_mod
                
                checkpoint = {
                    'network_weights': mod.state_dict(),
                    'optimizer_state': self.optimizer.state_dict(),
                    'grad_scaler_state': self.grad_scaler.state_dict() if self.grad_scaler is not None else None,
                    'logging': self.logger.get_checkpoint(),
                    '_best_ema': self._best_ema,
                    'current_epoch': self.current_epoch,
                    'init_args': self.my_init_kwargs,
                    'trainer_name': self.__class__.__name__,
                    'inference_allowed_mirroring_axes': self.inference_allowed_mirroring_axes,
                    'distill_params': {
                        'loss_type': self.distill_loss_type,
                        'temperature': self.distill_temperature,
                        'alpha': self.distill_alpha,
                        'beta': self.distill_beta,
                        'teacher_checkpoint': self.teacher_checkpoint,
                        'teacher_configuration': self.teacher_configuration
                    },
                    'lightweight_info': {
                        'is_lightweight': True,
                        'has_bottleneck_blocks': True,
                        'has_pyramid_pooling': True,
                        'bottleneck_ratio': 0.25
                    }
                }
                torch.save(checkpoint, filename)
                distill_logger.info(f"ğŸ’¾ è½»é‡çº§è’¸é¦æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")
            else:
                self.print_to_log_file('No checkpoint written, checkpointing is disabled')

    def load_checkpoint(self, filename_or_checkpoint):
        """ä¿®å¤torch.compileå‰ç¼€é—®é¢˜çš„æ£€æŸ¥ç‚¹åŠ è½½"""
        if isinstance(filename_or_checkpoint, str):
            if not filename_or_checkpoint.endswith('.pth'):
                filename_or_checkpoint += '.pth'
            filename = filename_or_checkpoint
            self.print_to_log_file(f"loading checkpoint {filename}")
            if not isfile(filename):
                raise RuntimeError(f"Cannot load {filename}")
            # ä¿®å¤PyTorch 2.6çš„weights_onlyé—®é¢˜
            checkpoint = torch.load(filename, map_location=torch.device('cpu'), weights_only=False)
        else:
            checkpoint = filename_or_checkpoint

        # ç¡®ä¿ç½‘ç»œå·²åˆå§‹åŒ–
        if self.network is None:
            distill_logger.warning("âš ï¸ ç½‘ç»œæœªåˆå§‹åŒ–ï¼Œå…ˆè°ƒç”¨initialize()")
            self.initialize()

        # ä¿®å¤æƒé‡å‰ç¼€é—®é¢˜
        network_weights = checkpoint['network_weights']
        
        # æ£€æµ‹å½“å‰ç½‘ç»œæ˜¯å¦æ˜¯compiledï¼Œä»¥åŠæƒé‡æ˜¯å¦æœ‰_orig_modå‰ç¼€
        is_network_compiled = isinstance(self.network, OptimizedModule) or hasattr(self.network, '_orig_mod')
        has_orig_mod_prefix = any(key.startswith('_orig_mod.') for key in network_weights.keys())
        
        if is_network_compiled and not has_orig_mod_prefix:
            # ç½‘ç»œæ˜¯compiledçš„ï¼Œä½†æƒé‡æ²¡æœ‰å‰ç¼€ï¼Œéœ€è¦æ·»åŠ å‰ç¼€
            compiled_weights = {}
            for key, value in network_weights.items():
                compiled_weights[f'_orig_mod.{key}'] = value
            self.network.load_state_dict(compiled_weights)
        elif not is_network_compiled and has_orig_mod_prefix:
            # ç½‘ç»œä¸æ˜¯compiledçš„ï¼Œä½†æƒé‡æœ‰å‰ç¼€ï¼Œéœ€è¦å»æ‰å‰ç¼€
            clean_weights = {}
            for key, value in network_weights.items():
                if key.startswith('_orig_mod.'):
                    clean_weights[key[10:]] = value
                else:
                    clean_weights[key] = value
            self.network.load_state_dict(clean_weights)
        else:
            # ç½‘ç»œå’Œæƒé‡åŒ¹é…ï¼Œç›´æ¥åŠ è½½
            self.network.load_state_dict(network_weights)

        self.current_epoch = checkpoint['current_epoch']
        if self.was_initialized:
            self.optimizer.load_state_dict(checkpoint['optimizer_state'])
            if self.grad_scaler is not None:
                if checkpoint['grad_scaler_state'] is not None:
                    self.grad_scaler.load_state_dict(checkpoint['grad_scaler_state'])

        if '_best_ema' in checkpoint.keys():
            self._best_ema = checkpoint['_best_ema']

        # Load distill params if available
        if 'distill_params' in checkpoint:
            distill_params = checkpoint['distill_params']
            self.distill_loss_type = distill_params.get('loss_type', 'kl')
            self.distill_temperature = distill_params.get('temperature', 3.0)
            self.distill_alpha = distill_params.get('alpha', 0.7)
            self.distill_beta = distill_params.get('beta', 0.3)
            
        # Initialize logger from checkpoint
        if hasattr(self.logger, 'load_checkpoint') and self.logger is not None:
            self.logger.load_checkpoint(checkpoint['logging'])

        # Set inference axes if available
        if 'inference_allowed_mirroring_axes' in checkpoint.keys():
            self.inference_allowed_mirroring_axes = checkpoint['inference_allowed_mirroring_axes']

        distill_logger.info(f"âœ… è’¸é¦æ¨¡å‹æ£€æŸ¥ç‚¹å·²åŠ è½½: epoch {self.current_epoch}")
        distill_logger.info(f"ğŸ”„ ç»§ç»­è½»é‡çº§è’¸é¦è®­ç»ƒ: Î±={self.distill_alpha}, T={self.distill_temperature}")


# ä¿®å¤ 1: å®šä¹‰ dummy_context
@contextmanager
def dummy_context():
    yield None


# ä¿®å¤ 2: å®šä¹‰ OptimizedModule
class OptimizedModule(torch.nn.Module):
    def __init__(self, original_module):
        super().__init__()
        self._orig_mod = original_module

    def forward(self, *args, **kwargs):
        return self._orig_mod(*args, **kwargs)


class LightweightKnowledgeDistillationLoss(nn.Module):
    """
    è½»é‡çº§çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°
    ä¸“é—¨ä¸ºè½»é‡çº§åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ä¼˜åŒ–
    """
    
    def __init__(self, loss_type='kl', temperature=3.0, alpha=0.7, beta=0.3):
        super().__init__()
        self.loss_type = loss_type
        self.temperature = temperature
        self.alpha = alpha
        self.beta = beta
        
        distill_logger.info(f"ğŸ¯ åˆå§‹åŒ–è½»é‡çº§è’¸é¦æŸå¤±: {loss_type}, T={temperature}, Î±={alpha}")
        
        # ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•ï¼šç»§æ‰¿çˆ¶ç±»çš„æŸå¤±å‡½æ•°
        # è¿™æ ·å¯ä»¥è‡ªåŠ¨å¤„ç†æ·±åº¦ç›‘ç£
        self.task_loss = None  # ç¨ååœ¨çˆ¶ç±»ä¸­åˆå§‹åŒ–
    
    def kl_divergence_loss(self, student_logits, teacher_logits):
        """é€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„KLæ•£åº¦æŸå¤± - æ•°å€¼ç¨³å®šç‰ˆæœ¬"""
        # å¤„ç†æ·±åº¦ç›‘ç£çš„æƒ…å†µ
        if isinstance(student_logits, (list, tuple)):
            student_logits = student_logits[0]  # ä½¿ç”¨ä¸»è¦è¾“å‡º
        if isinstance(teacher_logits, (list, tuple)):
            teacher_logits = teacher_logits[0]  # ä½¿ç”¨ä¸»è¦è¾“å‡º
            
        # ç¡®ä¿å½¢çŠ¶åŒ¹é…
        if student_logits.shape != teacher_logits.shape:
            teacher_logits = F.interpolate(
                teacher_logits, size=student_logits.shape[2:], 
                mode='trilinear', align_corners=False
            )
        
        # æ•°å€¼ç¨³å®šçš„softmaxè®¡ç®—
        student_scaled = student_logits / self.temperature
        teacher_scaled = teacher_logits / self.temperature
        
        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§
        student_scaled = torch.clamp(student_scaled, min=-10, max=10)
        teacher_scaled = torch.clamp(teacher_scaled, min=-10, max=10)
        
        soft_teacher = F.softmax(teacher_scaled, dim=1)
        log_soft_student = F.log_softmax(student_scaled, dim=1)
        
        kl_loss = F.kl_div(log_soft_student, soft_teacher, reduction='batchmean')
        
        # æ£€æŸ¥NaNå¹¶å¤„ç†
        if torch.isnan(kl_loss) or torch.isinf(kl_loss):
            return torch.tensor(0.0, device=kl_loss.device, requires_grad=True)
        
        # åªä¹˜ä»¥æ¸©åº¦ï¼Œä¸æ˜¯æ¸©åº¦çš„å¹³æ–¹ï¼Œé¿å…æŸå¤±è¿‡å¤§
        return kl_loss * self.temperature * 0.1  # é¢å¤–ç¼©æ”¾å› å­
    
    def mse_loss(self, student_logits, teacher_logits):
        """é€‚ç”¨äºä½“ç´ çº§é¢„æµ‹çš„MSEæŸå¤±"""
        if isinstance(student_logits, (list, tuple)):
            student_logits = student_logits[0]
        if isinstance(teacher_logits, (list, tuple)):
            teacher_logits = teacher_logits[0]
            
        # ç¡®ä¿å½¢çŠ¶åŒ¹é…
        if student_logits.shape != teacher_logits.shape:
            teacher_logits = F.interpolate(
                teacher_logits, size=student_logits.shape[2:], 
                mode='trilinear', align_corners=False
            )
            
        return F.mse_loss(student_logits, teacher_logits)
    
    def attention_transfer_loss(self, student_features, teacher_features):
        """é’ˆå¯¹è½»é‡çº§åˆ†å‰²ä»»åŠ¡çš„æ³¨æ„åŠ›è½¬ç§»æŸå¤±"""
        if not isinstance(student_features, (list, tuple)):
            student_features = [student_features]
        if not isinstance(teacher_features, (list, tuple)):
            teacher_features = [teacher_features]
            
        loss = 0
        valid_pairs = 0
        
        for s_feat, t_feat in zip(student_features, teacher_features):
            if s_feat is not None and t_feat is not None:
                s_att = self._spatial_attention_map(s_feat)
                t_att = self._spatial_attention_map(t_feat)
                
                if s_att.shape != t_att.shape:
                    t_att = F.interpolate(t_att, size=s_att.shape[2:], mode='trilinear', align_corners=False)
                
                loss += F.mse_loss(s_att, t_att)
                valid_pairs += 1
        
        return loss / max(valid_pairs, 1)
    
    def _spatial_attention_map(self, feature: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—3Dåˆ†å‰²ä»»åŠ¡çš„ç©ºé—´æ³¨æ„åŠ›å›¾"""
        return torch.norm(feature, p=2, dim=1, keepdim=True)
    
    def forward(self, student_output: torch.Tensor, 
                teacher_output: torch.Tensor, 
                target: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ€»æŸå¤± - é’ˆå¯¹è½»é‡çº§æ¨¡å‹ä¼˜åŒ–
        """
        # å¤„ç†æ·±åº¦ç›‘ç£è¾“å‡ºï¼ˆå¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ä¸»è¦è¾“å‡ºï¼‰
        if isinstance(student_output, (list, tuple)):
            student_main = student_output[0]
        else:
            student_main = student_output
            
        if isinstance(teacher_output, (list, tuple)):
            teacher_main = teacher_output[0]
        else:
            teacher_main = teacher_output
        
        # è®¡ç®—ä»»åŠ¡æŸå¤±ï¼ˆåˆ†å‰²æŸå¤±ï¼‰- å¦‚æœtask_lossä¸ºç©ºï¼Œè¿”å›0
        if self.task_loss is not None:
            task_loss = self.task_loss(student_output, target)
        else:
            # å¤„ç†ç›®æ ‡å¼ é‡çš„å½¢çŠ¶å’Œæ•°æ®ç±»å‹
            target_processed = target
            if isinstance(target, (list, tuple)):
                target_processed = target[0]
            
            # å¦‚æœç›®æ ‡å¼ é‡æœ‰å¤šä½™çš„ç»´åº¦ï¼Œå»æ‰å®ƒ
            if len(target_processed.shape) == 5 and target_processed.shape[1] == 1:
                target_processed = target_processed.squeeze(1)
            
            # ç¡®ä¿ç›®æ ‡å¼ é‡æ˜¯é•¿æ•´å‹ï¼ˆLongTensorï¼‰
            target_processed = target_processed.long()
            
            # ç®€å•çš„å¤‡ç”¨æŸå¤±
            if isinstance(student_output, (list, tuple)):
                task_loss = F.cross_entropy(student_output[0], target_processed)
            else:
                task_loss = F.cross_entropy(student_output, target_processed)
        
        # è®¡ç®—è’¸é¦æŸå¤± - ä½¿ç”¨ä¸»è¦è¾“å‡º
        if self.loss_type == 'attention':
            # å¦‚æœæœ‰ç‰¹å¾å›¾ï¼Œä½¿ç”¨æ³¨æ„åŠ›è½¬ç§»
            if (isinstance(student_output, (list, tuple)) and len(student_output) > 1 and
                isinstance(teacher_output, (list, tuple)) and len(teacher_output) > 1):
                distill_loss = self.attention_transfer_loss(student_output[1:], teacher_output[1:])
            else:
                # å›é€€åˆ°KLæ•£åº¦
                distill_loss = self.kl_divergence_loss(student_main, teacher_main)
        elif self.loss_type == 'mse':
            distill_loss = self.mse_loss(student_main, teacher_main)
        else:  # é»˜è®¤ä½¿ç”¨KLæ•£åº¦
            distill_loss = self.kl_divergence_loss(student_main, teacher_main)
        
        # ç»„åˆæŸå¤± - ä½¿ç”¨æå°çš„è’¸é¦æƒé‡ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        # é¦–å…ˆæ£€æŸ¥æŸå¤±çš„æœ‰æ•ˆæ€§
        if torch.isnan(task_loss) or torch.isinf(task_loss):
            task_loss = torch.tensor(1.0, device=task_loss.device, requires_grad=True)
        if torch.isnan(distill_loss) or torch.isinf(distill_loss):
            distill_loss = torch.tensor(0.0, device=distill_loss.device, requires_grad=True)
        
        # ä½¿ç”¨æå°çš„è’¸é¦æƒé‡ï¼Œä¼˜å…ˆä¿è¯ä»»åŠ¡æŸå¤±çš„ç¨³å®šæ€§
        alpha_safe = 0.001  # æå°çš„è’¸é¦æƒé‡
        total_loss = (1 - alpha_safe) * task_loss + alpha_safe * distill_loss
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            total_loss = task_loss  # å¦‚æœä»æœ‰é—®é¢˜ï¼Œåªç”¨ä»»åŠ¡æŸå¤±
        
        return total_loss


# å¯¼å…¥å¿…è¦çš„å‡½æ•°
def determine_num_input_channels(plans_manager, configuration_manager, dataset_json):
    """ç¡®å®šè¾“å…¥é€šé“æ•°"""
    return len(dataset_json['channel_names'])
