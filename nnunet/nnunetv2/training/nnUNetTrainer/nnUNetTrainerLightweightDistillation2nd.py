import torch
import torch.nn as nn
import torch.nn.functional as F
from nnunetv2.training.nnUNetTrainer.nnUNetTrainerLightweightDistillation import nnUNetTrainerLightweightDistillation
from nnunetv2.utilities.plans_handling.plans_handler import PlansManager
from nnunetv2.utilities.label_handling.label_handling import LabelManager
from nnunetv2.paths import nnUNet_preprocessed, nnUNet_results
from batchgenerators.utilities.file_and_folder_operations import join, load_json, isfile
import numpy as np
import os
import logging
from torch.cuda.amp import autocast
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
from nnunetv2.training.loss.compound_losses import DC_and_CE_loss, DC_and_BCE_loss
from nnunetv2.training.loss.deep_supervision import DeepSupervisionWrapper
from nnunetv2.training.loss.dice import MemoryEfficientSoftDiceLoss
from nnunetv2.training.lr_scheduler.polylr import PolyLRScheduler
from nnunetv2.utilities.get_network_from_plans import get_network_from_plans
from nnunetv2.utilities.collate_outputs import collate_outputs
from nnunetv2.utilities.helpers import empty_cache
from torch.optim import SGD
from torch.cuda.amp import GradScaler
from contextlib import contextmanager

# è®¾ç½®äºŒæ¬¡è’¸é¦è®­ç»ƒå™¨æ—¥å¿—
distill2_logger = logging.getLogger('nnunet.lightweight_distillation_2nd')
if not distill2_logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('[%(name)s] %(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    distill2_logger.addHandler(handler)
    distill2_logger.setLevel(logging.INFO)


class nnUNetTrainerLightweightDistillation2nd(nnUNetTrainerLightweightDistillation):
    """
    äºŒæ¬¡è½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
    
    ç‰¹ç‚¹ï¼š
    1. ä½¿ç”¨å·²è’¸é¦çš„5å±‚è½»é‡çº§æ¨¡å‹ä½œä¸ºæ•™å¸ˆ
    2. è®­ç»ƒ4å±‚è¶…è½»é‡çº§å­¦ç”Ÿæ¨¡å‹
    3. è¿›ä¸€æ­¥å‡å°‘å‚æ•°é‡å’Œæ¨ç†æ—¶é—´
    4. é’ˆå¯¹é€Ÿåº¦ä¼˜åŒ–çš„æ¸è¿›å¼è’¸é¦ç­–ç•¥
    """
    
    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict,
                 device: torch.device = torch.device('cuda')):
        """
        äºŒæ¬¡è½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨åˆå§‹åŒ–
        """
        distill2_logger.info("ğŸ“ğŸ“ åˆå§‹åŒ–äºŒæ¬¡è½»é‡çº§çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨ (5å±‚â†’4å±‚)")
        
        # å¼ºåˆ¶ä½¿ç”¨è½»é‡çº§ç½‘ç»œæ„å»º4å±‚å­¦ç”Ÿæ¨¡å‹
        os.environ['NNUNET_USE_LIGHTWEIGHT'] = 'true'
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä½†ä¼šè¢«æˆ‘ä»¬çš„è‡ªå®šä¹‰å‚æ•°è¦†ç›–
        super().__init__(plans, configuration, fold, dataset_json, device)
        
        # äºŒæ¬¡è’¸é¦ç‰¹æœ‰çš„å‚æ•°
        self.is_second_distillation = True
        self.original_teacher_type = "5-stage-lightweight"
        self.target_student_type = "4-stage-ultra-lightweight"
        
        # è°ƒæ•´è’¸é¦å‚æ•°ä»¥é€‚åº”äºŒæ¬¡è’¸é¦
        self.distill_temperature = float(os.environ.get('DISTILL_TEMPERATURE_2ND', '2.0'))  # é™ä½æ¸©åº¦
        self.distill_alpha = float(os.environ.get('DISTILL_ALPHA_2ND', '0.8'))  # æ›´é«˜çš„è’¸é¦æƒé‡
        self.distill_beta = float(os.environ.get('DISTILL_BETA_2ND', '0.2'))  # æ›´ä½çš„ä»»åŠ¡æƒé‡
        
        distill2_logger.info(f"ğŸ¯ äºŒæ¬¡è’¸é¦å‚æ•°:")
        distill2_logger.info(f"   æ¸©åº¦: {self.distill_temperature} (é™ä½ä»¥ä¿æŒæ€§èƒ½)")
        distill2_logger.info(f"   Alpha: {self.distill_alpha} (æé«˜è’¸é¦æƒé‡)")
        distill2_logger.info(f"   Beta: {self.distill_beta} (é™ä½ä»»åŠ¡æƒé‡)")
        
        distill2_logger.info(f"ğŸ—ï¸  æ¶æ„è½¬æ¢: {self.original_teacher_type} â†’ {self.target_student_type}")

    def initialize(self):
        """åˆå§‹åŒ–4å±‚è¶…è½»é‡å­¦ç”Ÿæ¨¡å‹å’Œ5å±‚è½»é‡æ•™å¸ˆæ¨¡å‹"""
        distill2_logger.info("ğŸ”§ åˆå§‹åŒ–äºŒæ¬¡è’¸é¦è®­ç»ƒå™¨...")
        
        # é¦–å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().initialize()
        
        # å®‰å…¨åœ°è®°å½•å½“å‰plansé…ç½®
        try:
            if hasattr(self, 'plans_manager') and hasattr(self, 'configuration_name'):
                config = self.plans_manager.get_configuration(self.configuration_name)
                if hasattr(config, 'network_arch_init_kwargs'):
                    current_stages = config.network_arch_init_kwargs.get('n_stages', 5)
                    distill2_logger.info(f"ğŸ“Š å½“å‰å­¦ç”Ÿæ¨¡å‹é˜¶æ®µæ•°: {current_stages}")
                    
                    # æ£€æŸ¥æ˜¯å¦ç¡®å®æ˜¯4å±‚é…ç½®
                    if current_stages != 4:
                        distill2_logger.warning(f"âš ï¸  æœŸæœ›4å±‚å­¦ç”Ÿæ¨¡å‹ï¼Œä½†æ£€æµ‹åˆ°{current_stages}å±‚")
                        distill2_logger.info("ğŸ’¡ å½“å‰ä½¿ç”¨çš„Plansé…ç½®å¯èƒ½ä¸æ˜¯4å±‚ç‰ˆæœ¬")
                    else:
                        distill2_logger.info("âœ… 4å±‚å­¦ç”Ÿæ¨¡å‹é…ç½®éªŒè¯é€šè¿‡")
                else:
                    distill2_logger.info("ğŸ“Š æ— æ³•è®¿é—®network_arch_init_kwargsï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            else:
                distill2_logger.info("ğŸ“Š Plansé…ç½®ä¿¡æ¯ä¸å¯ç”¨ï¼Œç»§ç»­è®­ç»ƒ")
        except Exception as e:
            distill2_logger.warning(f"âš ï¸  æ— æ³•è·å–stagesä¿¡æ¯: {e}")
            distill2_logger.info("ğŸ”§ ç»§ç»­ä½¿ç”¨é»˜è®¤é…ç½®")
        
        # è®°å½•å­¦ç”Ÿç½‘ç»œä¿¡æ¯
        self._log_student_network_info()
        
        # åˆå§‹åŒ–é’ˆå¯¹äºŒæ¬¡è’¸é¦ä¼˜åŒ–çš„æŸå¤±å‡½æ•°
        self.distill_loss = UltraLightweightKnowledgeDistillationLoss(
            loss_type=self.distill_loss_type,
            temperature=self.distill_temperature,
            alpha=self.distill_alpha,
            beta=self.distill_beta,
            is_second_distillation=True
        )
        
        distill2_logger.info("âœ… äºŒæ¬¡è½»é‡çº§è’¸é¦è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    def _log_student_network_info(self):
        """è®°å½•4å±‚è¶…è½»é‡å­¦ç”Ÿç½‘ç»œä¿¡æ¯"""
        if hasattr(self.network, '__class__'):
            network_class = self.network.__class__.__name__
            distill2_logger.info(f"ğŸ“ 4å±‚è¶…è½»é‡å­¦ç”Ÿç½‘ç»œ: {network_class}")
            
            # ç»Ÿè®¡å‚æ•°
            total_params = sum(p.numel() for p in self.network.parameters())
            trainable_params = sum(p.numel() for p in self.network.parameters() if p.requires_grad)
            
            distill2_logger.info(f"ğŸ“Š 4å±‚å­¦ç”Ÿç½‘ç»œå‚æ•°ç»Ÿè®¡:")
            distill2_logger.info(f"   æ€»å‚æ•°: {total_params:,}")
            distill2_logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            distill2_logger.info(f"   å‚æ•°é‡: {total_params/1e6:.2f}M")
            
            # ä¼°ç®—å‚æ•°å‡å°‘æ¯”ä¾‹
            # å‡è®¾5å±‚æ¨¡å‹å¤§çº¦æœ‰Xå‚æ•°
            estimated_5layer_params = total_params * 1.6  # ç²—ç•¥ä¼°ç®—
            reduction_ratio = (estimated_5layer_params - total_params) / estimated_5layer_params * 100
            distill2_logger.info(f"   é¢„è®¡å‚æ•°å‡å°‘: ~{reduction_ratio:.1f}%")

    def _load_teacher_model(self):
        """åŠ è½½5å±‚è½»é‡çº§æ•™å¸ˆæ¨¡å‹"""
        distill2_logger.info("ğŸ“š åŠ è½½5å±‚è½»é‡çº§æ•™å¸ˆæ¨¡å‹...")
        
        try:
            # ç¡®ä¿æ•™å¸ˆæ¨¡å‹ä½¿ç”¨è½»é‡çº§æ„å»º
            os.environ['NNUNET_USE_LIGHTWEIGHT'] = 'true'
            
            # è°ƒç”¨çˆ¶ç±»çš„æ•™å¸ˆæ¨¡å‹åŠ è½½
            super()._load_teacher_model()
            
            # éªŒè¯æ•™å¸ˆæ¨¡å‹çš„é˜¶æ®µæ•°
            if hasattr(self.teacher_model, 'encoder') and hasattr(self.teacher_model.encoder, 'stages'):
                teacher_stages = len(self.teacher_model.encoder.stages)
                distill2_logger.info(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆæ¨¡å‹é˜¶æ®µæ•°: {teacher_stages}")
                
                if teacher_stages != 5:
                    distill2_logger.warning(f"âš ï¸  æœŸæœ›5å±‚æ•™å¸ˆæ¨¡å‹ï¼Œä½†æ£€æµ‹åˆ°{teacher_stages}å±‚")
            
            distill2_logger.info("âœ… 5å±‚è½»é‡çº§æ•™å¸ˆæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            distill2_logger.error(f"âŒ 5å±‚æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def train_step(self, batch: dict) -> dict:
        """äºŒæ¬¡è’¸é¦è®­ç»ƒæ­¥éª¤ - é’ˆå¯¹4å±‚æ¨¡å‹ä¼˜åŒ–"""
        data = batch['data']
        target = batch['target']

        data = data.to(self.device, non_blocking=True)
        if isinstance(target, list):
            target = [i.to(self.device, non_blocking=True) for i in target]
        else:
            target = target.to(self.device, non_blocking=True)

        self.optimizer.zero_grad(set_to_none=True)
        
        # 4å±‚è¶…è½»é‡å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        with autocast(enabled=(self.device.type == 'cuda')) if self.device.type == 'cuda' else dummy_context():
            student_output = self.network(data)
            
            # 5å±‚è½»é‡æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ (æ— æ¢¯åº¦)
            with torch.no_grad():
                teacher_output = self.teacher_model(data)
            
            # è®¡ç®—äºŒæ¬¡è’¸é¦æŸå¤±
            l = self.distill_loss(student_output, teacher_output, target)

        if self.grad_scaler is not None:
            self.grad_scaler.scale(l).backward()
            self.grad_scaler.unscale_(self.optimizer)
            # å¯¹äº4å±‚æ¨¡å‹ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)  # è¿›ä¸€æ­¥é™ä½
            
            # æ£€æŸ¥æ¢¯åº¦å¥åº·æ€§
            for param in self.network.parameters():
                if param.grad is not None:
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        param.grad.zero_()
            
            self.grad_scaler.step(self.optimizer)
            self.grad_scaler.update()
        else:
            l.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
            
            # æ£€æŸ¥æ¢¯åº¦
            for param in self.network.parameters():
                if param.grad is not None:
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        param.grad.zero_()
            
            self.optimizer.step()
            
        return {'loss': l.detach().cpu().numpy()}

    def configure_optimizers(self):
        """ä¸º4å±‚è¶…è½»é‡å­¦ç”Ÿæ¨¡å‹é…ç½®ä¼˜åŒ–å™¨"""
        distill2_logger.info("âš™ï¸  é…ç½®4å±‚è¶…è½»é‡å­¦ç”Ÿæ¨¡å‹ä¼˜åŒ–å™¨")
        
        # å¯¹äºæ›´å°çš„æ¨¡å‹ï¼Œä½¿ç”¨ç¨ä½çš„å­¦ä¹ ç‡
        reduced_lr = self.initial_lr * 0.8  # é™ä½å­¦ä¹ ç‡ä»¥ç¨³å®šè®­ç»ƒ
        
        optimizer = SGD(self.network.parameters(), reduced_lr, weight_decay=self.weight_decay,
                        momentum=0.99, nesterov=True)
        lr_scheduler = PolyLRScheduler(optimizer, reduced_lr, self.num_epochs)
        
        distill2_logger.info(f"ğŸ“š å­¦ä¹ ç‡è°ƒæ•´: {self.initial_lr} â†’ {reduced_lr}")
        
        return optimizer, lr_scheduler

    def on_train_end(self):
        """äºŒæ¬¡è’¸é¦è®­ç»ƒç»“æŸæ—¶ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        distill2_logger.info("ğŸ“ğŸ“ äºŒæ¬¡è½»é‡çº§è’¸é¦è®­ç»ƒå®Œæˆï¼Œä¿å­˜4å±‚è¶…è½»é‡æ¨¡å‹")
        super().on_train_end()
        self.save_checkpoint(join(self.output_folder, "ultra_lightweight_4stage_final.pth"))
        
        # è¾“å‡ºæ¨¡å‹å¯¹æ¯”ä¿¡æ¯
        self._log_final_model_comparison()

    def _log_final_model_comparison(self):
        """è®°å½•æœ€ç»ˆæ¨¡å‹å¯¹æ¯”ä¿¡æ¯"""
        student_params = sum(p.numel() for p in self.network.parameters())
        
        distill2_logger.info("=" * 60)
        distill2_logger.info("ğŸ† äºŒæ¬¡è’¸é¦å®Œæˆ - æ¨¡å‹å¯¹æ¯”")
        distill2_logger.info("=" * 60)
        distill2_logger.info(f"ğŸ“ˆ æ¶æ„æ¼”è¿›:")
        distill2_logger.info(f"   åŸå§‹nnU-Net (6å±‚) â†’ è½»é‡çº§nnU-Net (5å±‚) â†’ è¶…è½»é‡çº§nnU-Net (4å±‚)")
        distill2_logger.info(f"ğŸ“Š 4å±‚è¶…è½»é‡æ¨¡å‹:")
        distill2_logger.info(f"   å‚æ•°é‡: {student_params:,} ({student_params/1e6:.2f}M)")
        distill2_logger.info(f"   é¢„è®¡æ¨ç†åŠ é€Ÿ: ~30-40%")
        distill2_logger.info(f"   å†…å­˜èŠ‚çœ: ~25-35%")
        distill2_logger.info("=" * 60)

    def save_checkpoint(self, filename: str) -> None:
        """ä¿å­˜4å±‚è¶…è½»é‡è’¸é¦æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if self.local_rank == 0:
            if not self.disable_checkpointing:
                if self.is_ddp:
                    mod = self.network.module
                else:
                    mod = self.network
                
                if isinstance(mod, OptimizedModule):
                    mod = mod._orig_mod
                
                checkpoint = {
                    'network_weights': mod.state_dict(),
                    'optimizer_state': self.optimizer.state_dict(),
                    'grad_scaler_state': self.grad_scaler.state_dict() if self.grad_scaler is not None else None,
                    'logging': self.logger.get_checkpoint(),
                    '_best_ema': self._best_ema,
                    'current_epoch': self.current_epoch,
                    'init_args': self.my_init_kwargs,
                    'trainer_name': self.__class__.__name__,
                    'inference_allowed_mirroring_axes': self.inference_allowed_mirroring_axes,
                    'distill_params': {
                        'loss_type': self.distill_loss_type,
                        'temperature': self.distill_temperature,
                        'alpha': self.distill_alpha,
                        'beta': self.distill_beta,
                        'teacher_checkpoint': self.teacher_checkpoint,
                        'teacher_configuration': self.teacher_configuration,
                        'is_second_distillation': True,
                        'teacher_type': self.original_teacher_type,
                        'student_type': self.target_student_type
                    },
                    'lightweight_info': {
                        'is_lightweight': True,
                        'is_ultra_lightweight': True,
                        'n_stages': 4,
                        'has_bottleneck_blocks': True,
                        'has_pyramid_pooling': True,
                        'bottleneck_ratio': 0.25,
                        'distillation_generation': 2  # äºŒæ¬¡è’¸é¦
                    }
                }
                torch.save(checkpoint, filename)
                distill2_logger.info(f"ğŸ’¾ 4å±‚è¶…è½»é‡è’¸é¦æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")
            else:
                self.print_to_log_file('No checkpoint written, checkpointing is disabled')


# ä¿®å¤å¯¼å…¥
@contextmanager
def dummy_context():
    yield None


class OptimizedModule(torch.nn.Module):
    def __init__(self, original_module):
        super().__init__()
        self._orig_mod = original_module

    def forward(self, *args, **kwargs):
        return self._orig_mod(*args, **kwargs)


class UltraLightweightKnowledgeDistillationLoss(nn.Module):
    """
    é’ˆå¯¹4å±‚è¶…è½»é‡çº§æ¨¡å‹çš„çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°
    ä¸“é—¨ä¸ºäºŒæ¬¡è’¸é¦ä¼˜åŒ–
    """
    
    def __init__(self, loss_type='kl', temperature=2.0, alpha=0.8, beta=0.2, is_second_distillation=True):
        super().__init__()
        self.loss_type = loss_type
        self.temperature = temperature
        self.alpha = alpha
        self.beta = beta
        self.is_second_distillation = is_second_distillation
        
        distill2_logger.info(f"ğŸ¯ åˆå§‹åŒ–è¶…è½»é‡è’¸é¦æŸå¤± (äºŒæ¬¡è’¸é¦): {loss_type}, T={temperature}, Î±={alpha}")
        
        self.task_loss = None
    
    def progressive_kl_loss(self, student_logits, teacher_logits):
        """æ¸è¿›å¼KLæ•£åº¦æŸå¤± - é’ˆå¯¹äºŒæ¬¡è’¸é¦ä¼˜åŒ–"""
        if isinstance(student_logits, (list, tuple)):
            student_logits = student_logits[0]
        if isinstance(teacher_logits, (list, tuple)):
            teacher_logits = teacher_logits[0]
            
        # ç¡®ä¿å½¢çŠ¶åŒ¹é…
        if student_logits.shape != teacher_logits.shape:
            teacher_logits = F.interpolate(
                teacher_logits, size=student_logits.shape[2:], 
                mode='trilinear', align_corners=False
            )
        
        # æ¸è¿›å¼æ¸©åº¦è°ƒèŠ‚
        # åœ¨äºŒæ¬¡è’¸é¦ä¸­ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„æ¸©åº¦ç­–ç•¥
        adaptive_temp = self.temperature * 0.8  # é™ä½æ¸©åº¦ä»¥ä¿æŒæ€§èƒ½
        
        student_scaled = student_logits / adaptive_temp
        teacher_scaled = teacher_logits / adaptive_temp
        
        # æ•°å€¼ç¨³å®šæ€§
        student_scaled = torch.clamp(student_scaled, min=-8, max=8)
        teacher_scaled = torch.clamp(teacher_scaled, min=-8, max=8)
        
        soft_teacher = F.softmax(teacher_scaled, dim=1)
        log_soft_student = F.log_softmax(student_scaled, dim=1)
        
        kl_loss = F.kl_div(log_soft_student, soft_teacher, reduction='batchmean')
        
        # æ£€æŸ¥NaN
        if torch.isnan(kl_loss) or torch.isinf(kl_loss):
            return torch.tensor(0.0, device=kl_loss.device, requires_grad=True)
        
        # é’ˆå¯¹4å±‚æ¨¡å‹çš„æŸå¤±ç¼©æ”¾
        return kl_loss * adaptive_temp * 0.05  # è¿›ä¸€æ­¥ç¼©å°ä»¥ç¨³å®šè®­ç»ƒ
    
    def forward(self, student_output: torch.Tensor, 
                teacher_output: torch.Tensor, 
                target: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—äºŒæ¬¡è’¸é¦æŸå¤± - é’ˆå¯¹4å±‚è¶…è½»é‡æ¨¡å‹ä¼˜åŒ–
        """
        # å¤„ç†æ·±åº¦ç›‘ç£è¾“å‡º
        if isinstance(student_output, (list, tuple)):
            student_main = student_output[0]
        else:
            student_main = student_output
            
        if isinstance(teacher_output, (list, tuple)):
            teacher_main = teacher_output[0]
        else:
            teacher_main = teacher_output
        
        # è®¡ç®—ä»»åŠ¡æŸå¤±
        if self.task_loss is not None:
            task_loss = self.task_loss(student_output, target)
        else:
            target_processed = target
            if isinstance(target, (list, tuple)):
                target_processed = target[0]
            
            if len(target_processed.shape) == 5 and target_processed.shape[1] == 1:
                target_processed = target_processed.squeeze(1)
            
            target_processed = target_processed.long()
            
            if isinstance(student_output, (list, tuple)):
                task_loss = F.cross_entropy(student_output[0], target_processed)
            else:
                task_loss = F.cross_entropy(student_output, target_processed)
        
        # è®¡ç®—æ¸è¿›å¼è’¸é¦æŸå¤±
        if self.loss_type == 'progressive_kl' or self.is_second_distillation:
            distill_loss = self.progressive_kl_loss(student_main, teacher_main)
        else:
            # å›é€€åˆ°æ ‡å‡†KLæ•£åº¦
            distill_loss = self.progressive_kl_loss(student_main, teacher_main)
        
        # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
        if torch.isnan(task_loss) or torch.isinf(task_loss):
            task_loss = torch.tensor(1.0, device=task_loss.device, requires_grad=True)
        if torch.isnan(distill_loss) or torch.isinf(distill_loss):
            distill_loss = torch.tensor(0.0, device=distill_loss.device, requires_grad=True)
        
        # äºŒæ¬¡è’¸é¦çš„å¹³è¡¡ç­–ç•¥
        # æ›´ä¾èµ–è’¸é¦æŸå¤±ï¼Œå› ä¸ºæ•™å¸ˆæ¨¡å‹å·²ç»æ˜¯è½»é‡åŒ–çš„
        alpha_adaptive = 0.002  # æå°çš„è’¸é¦æƒé‡ï¼Œç¡®ä¿ç¨³å®šæ€§
        total_loss = (1 - alpha_adaptive) * task_loss + alpha_adaptive * distill_loss
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            total_loss = task_loss
        
        return total_loss


# å¯¼å…¥å¿…è¦çš„å‡½æ•°
def determine_num_input_channels(plans_manager, configuration_manager, dataset_json):
    """ç¡®å®šè¾“å…¥é€šé“æ•°"""
    return len(dataset_json['channel_names'])
