import logging
from typing import Union, Tuple, List
import torch
from os.path import isfile
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer

# è®¾ç½®è½»é‡çº§è®­ç»ƒå™¨æ—¥å¿—
lightweight_trainer_logger = logging.getLogger('nnunet.lightweight_trainer')
if not lightweight_trainer_logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('[%(name)s] %(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    lightweight_trainer_logger.addHandler(handler)
    lightweight_trainer_logger.setLevel(logging.INFO)


class nnUNetTrainerLightweight(nnUNetTrainer):
    """
    è½»é‡çº§nnUNetè®­ç»ƒå™¨ï¼Œä½¿ç”¨è½»é‡çº§ç½‘ç»œç»„ä»¶æå‡è®­ç»ƒå’Œæ¨ç†æ•ˆç‡
    
    ä¸»è¦ä¼˜åŒ–:
    1. ä½¿ç”¨è½»é‡çº§ç¼–ç å™¨ç»„ä»¶
    2. æ·±åº¦å¯åˆ†ç¦»å·ç§¯å‡å°‘å‚æ•°é‡
    3. ç“¶é¢ˆæ®‹å·®å—æå‡æ•ˆç‡
    4. é‡‘å­—å¡”æ± åŒ–å¢å¼ºå¤šå°ºåº¦ç‰¹å¾
    """
    
    # æ·»åŠ è½»é‡çº§æ ‡è¯†ç¬¦
    __lightweight_trainer__ = True
    
    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict,
                 device: torch.device = torch.device('cuda')):
        # è®¾ç½®ç¯å¢ƒå˜é‡æ ‡è¯†è½»é‡çº§æ¨¡å¼
        import os
        os.environ['NNUNET_USE_LIGHTWEIGHT'] = 'true'
        
        super().__init__(plans, configuration, fold, dataset_json, device)
        
        lightweight_trainer_logger.info("ğŸš€ åˆå§‹åŒ– nnUNetTrainerLightweight - å¯ç”¨è½»é‡çº§ç½‘ç»œæ¶æ„")
        lightweight_trainer_logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_json.get('name', 'Unknown')}")
        lightweight_trainer_logger.info(f"ğŸ”§ é…ç½®: {configuration}, æŠ˜: {fold}")
        lightweight_trainer_logger.info("âš¡ è½»é‡çº§ç»„ä»¶å°†è‡ªåŠ¨æ¿€æ´»: ç“¶é¢ˆæ®‹å·®å— + é‡‘å­—å¡”æ± åŒ–")
    
    def initialize(self):
        """
        é‡å†™åˆå§‹åŒ–æ–¹æ³•ä»¥æ·»åŠ è½»é‡çº§ç»„ä»¶ä¿¡æ¯
        """
        lightweight_trainer_logger.info("ğŸ”§ åˆå§‹åŒ–è½»é‡çº§è®­ç»ƒå™¨...")
        super().initialize()
        
        # è®°å½•ç½‘ç»œå‚æ•°ç»Ÿè®¡
        if hasattr(self, 'network') and self.network is not None:
            total_params = sum(p.numel() for p in self.network.parameters())
            trainable_params = sum(p.numel() for p in self.network.parameters() if p.requires_grad)
            
            lightweight_trainer_logger.info(f"ğŸ“Š ç½‘ç»œå‚æ•°ç»Ÿè®¡:")
            lightweight_trainer_logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
            lightweight_trainer_logger.info(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
        
        lightweight_trainer_logger.info("âœ… è½»é‡çº§è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def on_train_start(self):
        """
        è®­ç»ƒå¼€å§‹æ—¶çš„å›è°ƒ
        """
        if hasattr(super(), 'on_train_start'):
            super().on_train_start()
        lightweight_trainer_logger.info("ğŸš€ å¼€å§‹è½»é‡çº§nnUNetè®­ç»ƒ")
        lightweight_trainer_logger.info("âš¡ è½»é‡çº§ç½‘ç»œæ¶æ„å·²æ¿€æ´»: ç“¶é¢ˆæ®‹å·®å— + æ·±åº¦å¯åˆ†ç¦»å·ç§¯ + é‡‘å­—å¡”æ± åŒ–")
        
        # éªŒè¯è½»é‡çº§ç½‘ç»œæ˜¯å¦æ­£ç¡®åŠ è½½
        if hasattr(self, 'network') and hasattr(self.network, '__class__'):
            network_class = self.network.__class__.__name__
            if 'Lightweight' in network_class:
                lightweight_trainer_logger.info(f"âœ… ç¡®è®¤ä½¿ç”¨è½»é‡çº§ç½‘ç»œ: {network_class}")
            else:
                lightweight_trainer_logger.warning(f"âš ï¸  æ£€æµ‹åˆ°æ ‡å‡†ç½‘ç»œ: {network_class}, è¯·æ£€æŸ¥é…ç½®")
    
    def on_epoch_start(self):
        """
        æ¯ä¸ªepochå¼€å§‹æ—¶çš„å›è°ƒ
        """
        if hasattr(super(), 'on_epoch_start'):
            super().on_epoch_start()
        if hasattr(self, 'current_epoch') and self.current_epoch % 20 == 0:  # æ¯20ä¸ªepochè®°å½•ä¸€æ¬¡
            lightweight_trainer_logger.info(f"âš¡ Epoch {self.current_epoch}: è½»é‡çº§ç½‘ç»œè®­ç»ƒä¸­...")
    
    def save_checkpoint(self, filename: str) -> None:
        """
        é‡å†™ä¿å­˜æ£€æŸ¥ç‚¹æ–¹æ³•ä»¥æ·»åŠ è½»é‡çº§æ ‡è¯†
        """
        super().save_checkpoint(filename)
        lightweight_trainer_logger.info(f"ğŸ’¾ è½»é‡çº§è®­ç»ƒå™¨æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")
    
    def load_checkpoint(self, filename_or_checkpoint):
        """
        é‡å†™åŠ è½½æ£€æŸ¥ç‚¹æ–¹æ³•ï¼Œä¿®å¤torch.compileå‰ç¼€å’ŒPyTorch 2.6 weights_onlyé—®é¢˜
        """
        if isinstance(filename_or_checkpoint, str):
            if not filename_or_checkpoint.endswith('.pth'):
                filename_or_checkpoint += '.pth'
            filename = filename_or_checkpoint
            self.print_to_log_file(f"loading checkpoint {filename}")
            if not isfile(filename):
                raise RuntimeError(f"Cannot load {filename}")
            # ä¿®å¤PyTorch 2.6çš„weights_onlyé—®é¢˜
            checkpoint = torch.load(filename, map_location=torch.device('cpu'), weights_only=False)
        else:
            checkpoint = filename_or_checkpoint

        # ä¿®å¤æƒé‡å‰ç¼€é—®é¢˜
        network_weights = checkpoint['network_weights']
        
        # æ£€æµ‹å½“å‰ç½‘ç»œæ˜¯å¦æ˜¯compiledï¼Œä»¥åŠæƒé‡æ˜¯å¦æœ‰_orig_modå‰ç¼€
        is_network_compiled = isinstance(self.network, OptimizedModule) or hasattr(self.network, '_orig_mod')
        has_orig_mod_prefix = any(key.startswith('_orig_mod.') for key in network_weights.keys())
        
        if is_network_compiled and not has_orig_mod_prefix:
            # ç½‘ç»œæ˜¯compiledçš„ï¼Œä½†æƒé‡æ²¡æœ‰å‰ç¼€ï¼Œéœ€è¦æ·»åŠ å‰ç¼€
            compiled_weights = {}
            for key, value in network_weights.items():
                compiled_weights[f'_orig_mod.{key}'] = value
            self.network.load_state_dict(compiled_weights)
        elif not is_network_compiled and has_orig_mod_prefix:
            # ç½‘ç»œä¸æ˜¯compiledçš„ï¼Œä½†æƒé‡æœ‰å‰ç¼€ï¼Œéœ€è¦å»æ‰å‰ç¼€
            clean_weights = {}
            for key, value in network_weights.items():
                if key.startswith('_orig_mod.'):
                    clean_weights[key[10:]] = value
                else:
                    clean_weights[key] = value
            self.network.load_state_dict(clean_weights)
        else:
            # ç½‘ç»œå’Œæƒé‡åŒ¹é…ï¼Œç›´æ¥åŠ è½½
            self.network.load_state_dict(network_weights)

        self.current_epoch = checkpoint['current_epoch']
        if self.was_initialized:
            self.optimizer.load_state_dict(checkpoint['optimizer_state'])
            if self.grad_scaler is not None:
                if checkpoint['grad_scaler_state'] is not None:
                    self.grad_scaler.load_state_dict(checkpoint['grad_scaler_state'])

        if '_best_ema' in checkpoint.keys():
            self._best_ema = checkpoint['_best_ema']

        # Initialize logger from checkpoint
        if hasattr(self.logger, 'load_checkpoint') and self.logger is not None:
            self.logger.load_checkpoint(checkpoint['logging'])

        # Set inference axes if available
        if 'inference_allowed_mirroring_axes' in checkpoint.keys():
            self.inference_allowed_mirroring_axes = checkpoint['inference_allowed_mirroring_axes']

        lightweight_trainer_logger.info(f"âœ… è½»é‡çº§è®­ç»ƒå™¨æ£€æŸ¥ç‚¹å·²åŠ è½½: epoch {self.current_epoch}")


# ä¿®å¤torch.compileå…¼å®¹æ€§é—®é¢˜
class OptimizedModule(torch.nn.Module):
    """å…¼å®¹æ€§ç±»ï¼Œç”¨äºå¤„ç†torch.compileäº§ç”Ÿçš„æ¨¡å—"""
    def __init__(self, original_module):
        super().__init__()
        self._orig_mod = original_module
        
    def __getattr__(self, name):
        if name == '_orig_mod':
            return super().__getattr__(name)
        return getattr(self._orig_mod, name)
